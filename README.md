Language identification has been of great importance for most NLP models today. However, as NLP models continue to improve, the underperformance of multilingual LLMs on non-English texts is still persistent. In particular, the existence of local slang phrases that are used orally instead of being formally written or taught is minimal in the training and testing datasets. This leads to the issue of inclusion if the language has dialects or informal phrases in their everyday use. Thus, we aim to present a model that correctly identifies the language on the input of foreign conventional phrases. To do so, we trained both the Naive Bayes and BERT models on Spanish, French, and Italian language subtitles from various movies and TV shows. By doing so, this project allowed us to better practice text classification by avoiding biases and implementing a model with better performance on non-English informal languages.
