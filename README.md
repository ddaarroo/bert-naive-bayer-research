Language identification has been of great impor-
tance for most NLP models today. However,
as NLP models continue to improve, the under-
performance of multilingual LLMs on non-English
texts is still persistent. In particular, the existence
of local slang phrases that are used orally instead
of being formally written or taught is minimal in
the training and testing datasets. This leads to the
issue of inclusion if the language has dialects or in-
formal phrases in their everyday use. Thus, we aim
to present a model that correctly identifies the lan-
guage on the input of foreign conventional phrases.
To do so, we trained both the Naive Bayes and
BERT models on Spanish, French, and Italian lan-
guage subtitles from various movies and TV shows.
By doing so, this project allowed us to better prac-
tice text classification by avoiding biases and im-
plementing a model with better performance on
non-English informal languages.
